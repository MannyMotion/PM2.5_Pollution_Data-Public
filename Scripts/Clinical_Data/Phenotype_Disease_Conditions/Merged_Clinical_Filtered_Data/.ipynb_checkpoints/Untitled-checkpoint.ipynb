{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7299d3c-c70a-4208-a95c-5a74a8c3cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json                           # filesystem + metadata\n",
    "import numpy as np                        # numerics\n",
    "import pandas as pd                       # dataframes\n",
    "from sklearn.model_selection import train_test_split   # split before transforms (anti-leakage)\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump                   # persist fitted preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a61ca88-775e-4e69-a161-8c0bc0f4b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns to *preserve* for merging later, but EXCLUDE from model features\n",
    "POSSIBLE_ID_COLS = [\n",
    "    \"Area Code\", \"Area Name\", \"PatientID\", \"StudyID\", \"Indicator ID\", \"Parent Code\"\n",
    "]\n",
    "\n",
    "#Aggregated numeric fields that must be non-negative if present\n",
    "NONNEGATIVE_COLS = [\n",
    "    \"Value\", \"Count\", \"Denominator\",\n",
    "    \"Lower CI 95.0 limit\", \"Upper CI 95.0 limit\",\n",
    "    \"Lower CI 99.8 limit\", \"Upper CI 99.8 limit\",\n",
    "]\n",
    "\n",
    "#Numeric columns that behave like categories/flags (encode, don't scale)\n",
    "LIKELY_NUMERIC_CATEGORICAL = [\n",
    "    \"New data\", \"Compared to goal\", \"Time period Sortable\"\n",
    "]\n",
    "\n",
    "ROW_MISSING_THRESHOLD = 0.80# drop rows if >80% of NON-ID columns missing\n",
    "TEST_SIZE = 0.20# 80/20 split\n",
    "RANDOM_STATE = 42# reproducibility\n",
    "SHUFFLE = True# usually True for i.i.d. tabular data\n",
    "\n",
    "#Outputs\n",
    "OUT_DIR = \"clinical_preml_outputs\"\n",
    "TRAIN_CSV = os.path.join(OUT_DIR, \"clinical_train_processed.csv\")\n",
    "TEST_CSV  = os.path.join(OUT_DIR, \"clinical_test_processed.csv\")\n",
    "TRAIN_IDS_CSV = os.path.join(OUT_DIR, \"clinical_train_ids.csv\")\n",
    "TEST_IDS_CSV  = os.path.join(OUT_DIR, \"clinical_test_ids.csv\")\n",
    "METADATA_JSON = os.path.join(OUT_DIR, \"clinical_processing_metadata.json\")\n",
    "PREPROCESSOR_JOBLIB = os.path.join(OUT_DIR, \"preprocessor.joblib\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)#ensure output folder exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "475f0351-d343-4c08-a4f1-229915d0d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) LOAD DATA\n",
    "merged_c_file_path = \"merged_clinical_data_V1.csv\"\n",
    "df_raw = pd.read_csv(merged_c_file_path)#read CSV once\n",
    "meta = {\"Initial shape\": list(df_raw.shape)}#record starting shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30efc9b8-c976-4d58-9e2f-b3cfd56807ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) DROP empty & constant columns (Rule 1)\n",
    "empty_cols = df_raw.columns[df_raw.isna().all()].tolist()  # columns all NaN\n",
    "df = df_raw.drop(columns=empty_cols)          # drop fully empty columns\n",
    "\n",
    "# drop columns with no variance (single unique across all rows)\n",
    "constant_cols = [c for c in df.columns if df[c].nunique(dropna=False) <= 1]\n",
    "df = df.drop(columns=constant_cols)\n",
    "\n",
    "# log what we removed\n",
    "meta[\"dropped_empty_columns\"] = empty_cols\n",
    "meta[\"dropped_constant_columns\"] = constant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a65f1d3-ddb8-4799-83af-47d9e5c7da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) REMOVE exact duplicates (Rule 4)\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()#exact row duplicates only\n",
    "meta[\"removed_duplicate_rows\"] = before - len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a49d8c0d-14b2-4855-aab7-d70351ede3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Identify ID columns (Rules 1,3,12)\n",
    "present_id_cols = [c for c in POSSIBLE_ID_COLS if c in df.columns]   # keep known IDs\n",
    "\n",
    "# Heuristic: treat ~unique columns as IDs too (avoid leaking identifiers as features)\n",
    "n = len(df)\n",
    "for c in df.columns:\n",
    "    if c not in present_id_cols:\n",
    "        uniq = df[c].nunique(dropna=True)\n",
    "        if uniq > 0.8 * n:               # ~unique per row → likely an identifier/timestamp\n",
    "            present_id_cols.append(c)\n",
    "\n",
    "present_id_cols = sorted(set(present_id_cols))  # de-dup and sort for stability\n",
    "meta[\"id_columns_kept\"] = present_id_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e448c9b-8067-412c-8ebf-7eb35990f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Logical range checks (Rule 5)\n",
    "for col in NONNEGATIVE_COLS:\n",
    "    if col in df.columns:\n",
    "        neg_n = int((df[col] < 0).sum())\n",
    "        if neg_n > 0:\n",
    "            df.loc[df[col] < 0, col] = np.nan\n",
    "        meta[f\"set_negative_to_nan__{col}\"] = neg_n\n",
    "\n",
    "# Ensure CI lower ≤ upper; if inverted, set both to NaN (conservative)\n",
    "if {\"Lower CI 95.0 limit\", \"Upper CI 95.0 limit\"}.issubset(df.columns):\n",
    "    mask = df[\"Lower CI 95.0 limit\"] > df[\"Upper CI 95.0 limit\"]\n",
    "    meta[\"ci95_inverted_pairs_set_nan\"] = int(mask.sum())\n",
    "    df.loc[mask, [\"Lower CI 95.0 limit\", \"Upper CI 95.0 limit\"]] = np.nan\n",
    "\n",
    "if {\"Lower CI 99.8 limit\", \"Upper CI 99.8 limit\"}.issubset(df.columns):\n",
    "    mask = df[\"Lower CI 99.8 limit\"] > df[\"Upper CI 99.8 limit\"]\n",
    "    meta[\"ci998_inverted_pairs_set_nan\"] = int(mask.sum())\n",
    "    df.loc[mask, [\"Lower CI 99.8 limit\", \"Upper CI 99.8 limit\"]] = np.nan\n",
    "\n",
    "#Ensures biological plausibility:\n",
    "#Negative values in numeric health metrics (e.g., blood pressure, counts) are set to NaN (to be imputed later).\n",
    "#If confidence interval lower bound > upper bound, both are set to NaN (data entry error).\n",
    "#Logs the number of fixes applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afc5766b-e050-4dd3-8a22-0b51b588e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Minimal categorical tidy (Rule 6)\n",
    "# Keep regional codes as-is (e.g., Sex), just trim whitespace and convert empty strings to NaN\n",
    "for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[c] = df[c].astype(str).str.strip().replace({\"\": np.nan})\n",
    "\n",
    "#Cleans string columns:\n",
    "#Strips extra spaces.\n",
    "#Converts empty strings to NaN.\n",
    "#No remapping of regional categorical codes (keeps “Sex” as local format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f27fb565-1fa2-4305-8d6b-ef7ce12f9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Drop rows with >80% missing across NON-ID columns (Rule 2)\n",
    "feature_candidates = [c for c in df.columns if c not in present_id_cols]  # only features for this test\n",
    "row_missing_frac = df[feature_candidates].isna().mean(axis=1)             # fraction missing per row\n",
    "drop_mask = row_missing_frac > ROW_MISSING_THRESHOLD                      # rows exceeding threshold\n",
    "meta[\"dropped_rows_gt80pct_missing\"] = int(drop_mask.sum())\n",
    "df = df.loc[~drop_mask].reset_index(drop=True)                            # keep good rows\n",
    "meta[\"shape_after_prefilter\"] = list(df.shape)\n",
    "\n",
    "#If a patient record is mostly empty (>80% missing in non-ID columns), it’s dropped (too incomplete to be useful).\n",
    "#Logs how many rows were removed and the new shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed73996b-7647-4d48-80c4-acff426e9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Train/Test split BEFORE any fit (Rule 8)\n",
    "# If you later add a target y, split X,y and consider stratify=y. Here we prep features only.\n",
    "X_all = df.copy()                                          # keep everything; IDs separated below\n",
    "\n",
    "X_train, X_test = train_test_split(\n",
    "    X_all, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=SHUFFLE\n",
    ")\n",
    "meta[\"train_shape_before_processing\"] = list(X_train.shape)\n",
    "meta[\"test_shape_before_processing\"]  = list(X_test.shape)\n",
    "\n",
    "# Separate and save IDs (kept for merging; excluded from ML features)\n",
    "train_ids = X_train[present_id_cols].copy() if present_id_cols else pd.DataFrame(index=X_train.index)\n",
    "test_ids  = X_test[present_id_cols].copy()  if present_id_cols else pd.DataFrame(index=X_test.index)\n",
    "\n",
    "# Remove IDs from features (avoid leakage)\n",
    "X_train = X_train.drop(columns=present_id_cols, errors=\"ignore\")\n",
    "X_test  = X_test.drop(columns=present_id_cols, errors=\"ignore\")\n",
    "\n",
    "#Split dataset into train/test before imputing, scaling, or encoding to prevent data leakage.\n",
    "#Save ID columns separately (so they’re not used in training) but keeps them for later merging with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e55704a-2b82-49b8-b67d-7d35ff377f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Feature typing (Rule 7)\n",
    "# Identify numeric vs text features on TRAIN ONLY (avoids peeking at test)\n",
    "num_all = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_text = X_train.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "\n",
    "# Split numeric into continuous vs numeric-categorical by simple cardinality heuristic\n",
    "numeric_categorical, numeric_continuous = [], []\n",
    "for c in num_all:\n",
    "    low_card = X_train[c].nunique(dropna=True) <= 20 or X_train[c].nunique(dropna=True) <= 0.01 * len(X_train)\n",
    "    if c in LIKELY_NUMERIC_CATEGORICAL or low_card:\n",
    "        numeric_categorical.append(c)         # will one-hot encode\n",
    "    else:\n",
    "        numeric_continuous.append(c)          # will impute + MinMax scale\n",
    "\n",
    "meta[\"numeric_continuous_cols\"] = numeric_continuous\n",
    "meta[\"numeric_categorical_cols\"] = numeric_categorical\n",
    "meta[\"categorical_text_cols\"]    = cat_text\n",
    "\n",
    "#Split numeric columns into:\n",
    "#1 Continuous numeric (real-valued measurements → scale)\n",
    "#2 Numeric categorical (code-like → one-hot encode)\n",
    "#All string columns are categorical text.\n",
    "#Logs the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1f793c9-9779-4304-8be9-a5ba1baa8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Build preprocessors (Rules 9, 10, 11)\n",
    "# Numeric continuous: median impute → MinMax scale to [0,1] (you requested min-max)\n",
    "num_cont_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\",  MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Numeric categorical (codes/flags): mode impute → OHE\n",
    "num_cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\",     OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "# Text categorical: mode impute → OHE\n",
    "txt_cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\",     OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine into a single column transformer; drop anything not specified\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num_cont\", num_cont_pipe, numeric_continuous),\n",
    "        (\"num_cat\",  num_cat_pipe,  numeric_categorical),\n",
    "        (\"txt_cat\",  txt_cat_pipe,  cat_text),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# Fit ONLY on training data; transform train and test\n",
    "Xtr = preprocessor.fit_transform(X_train)   # learn medians, mins/maxes, categories\n",
    "Xte = preprocessor.transform(X_test)        # apply learned params to test set\n",
    "\n",
    "#Continuous numeric: Median imputation → Min–Max scaling (0–1).\n",
    "#Numeric categorical & text categorical: Mode imputation → One-hot encoding.\n",
    "#ColumnTransformer applies the right pipeline to each feature type.\n",
    "#Fits only on training data, applies learned transformations to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "829e7ae7-d70d-4b0d-b275-0e20aee193f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Recover feature names for OHE blocks\n",
    "def get_feature_names(prep, num_cont, num_cat, txt_cat):\n",
    "    names = []\n",
    "    # continuous numeric: names unchanged by scaler\n",
    "    names += num_cont\n",
    "    # numeric-categorical (OHE): expand with categories\n",
    "    if num_cat:\n",
    "        names += prep.named_transformers_[\"num_cat\"].named_steps[\"ohe\"] \\\n",
    "                 .get_feature_names_out(num_cat).tolist()\n",
    "    # text-categorical (OHE): expand with categories\n",
    "    if txt_cat:\n",
    "        names += prep.named_transformers_[\"txt_cat\"].named_steps[\"ohe\"] \\\n",
    "                 .get_feature_names_out(txt_cat).tolist()\n",
    "    return names\n",
    "\n",
    "feature_names = get_feature_names(preprocessor, numeric_continuous, numeric_categorical, cat_text)\n",
    "\n",
    "# Convert arrays back to DataFrames (index preserved)\n",
    "Xtr_df = pd.DataFrame(Xtr, columns=feature_names, index=X_train.index)\n",
    "Xte_df = pd.DataFrame(Xte, columns=feature_names, index=X_test.index)\n",
    "\n",
    "#One-hot encoding creates new columns (one per category).\n",
    "#This step recovers human-readable column names and puts the arrays back into DataFrames with original row indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e134259a-efaa-4e29-816c-dff9331d8ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processing complete.\n",
      "Train features → clinical_preml_outputs\\clinical_train_processed.csv\n",
      "Test  features → clinical_preml_outputs\\clinical_test_processed.csv\n",
      "Train IDs     → clinical_preml_outputs\\clinical_train_ids.csv\n",
      "Test  IDs     → clinical_preml_outputs\\clinical_test_ids.csv\n",
      "Metadata       → clinical_preml_outputs\\clinical_processing_metadata.json\n",
      "Preprocessor   → clinical_preml_outputs\\preprocessor.joblib\n"
     ]
    }
   ],
   "source": [
    "# 12) SAVE outputs + metadata (Rules 12–14)\n",
    "# Save processed feature matrices\n",
    "Xtr_df.to_csv(TRAIN_CSV, index=False)\n",
    "Xte_df.to_csv(TEST_CSV,  index=False)\n",
    "\n",
    "# Save ID maps (for future merging with genetics/environment)\n",
    "if not train_ids.empty:\n",
    "    train_ids.to_csv(TRAIN_IDS_CSV, index=False)\n",
    "if not test_ids.empty:\n",
    "    test_ids.to_csv(TEST_IDS_CSV, index=False)\n",
    "\n",
    "# Record shapes and initial missingness (train only)\n",
    "meta.update({\n",
    "    \"final_train_shape\": list(Xtr_df.shape),\n",
    "    \"final_test_shape\":  list(Xte_df.shape),\n",
    "    \"initial_missing_train_counts\": X_train.isna().sum().to_dict()\n",
    "})\n",
    "\n",
    "# Persist metadata + fitted preprocessor for reproducibility / inference\n",
    "with open(METADATA_JSON, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "dump(preprocessor, PREPROCESSOR_JOBLIB)\n",
    "\n",
    "print(\"✅ Processing complete.\")\n",
    "print(f\"Train features → {TRAIN_CSV}\")\n",
    "print(f\"Test  features → {TEST_CSV}\")\n",
    "if present_id_cols:\n",
    "    print(f\"Train IDs     → {TRAIN_IDS_CSV}\")\n",
    "    print(f\"Test  IDs     → {TEST_IDS_CSV}\")\n",
    "print(f\"Metadata       → {METADATA_JSON}\")\n",
    "print(f\"Preprocessor   → {PREPROCESSOR_JOBLIB}\")\n",
    "\n",
    "#Saves processed training and test features.\n",
    "#Saves IDs separately for merging later.\n",
    "#Saves metadata JSON (audit trail of transformations, dropped rows/cols, etc.).\n",
    "#Saves the fitted preprocessor (so you can apply the exact same transformations to new incoming data at inference time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12494cd2-f8bb-4078-9ae7-daa912fe6cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5cb8e4-faf3-40f4-aa95-9fe69b42ef8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da9e88-af2b-4d9e-ae7e-9573696e8780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503641d-d276-488a-8175-126446c471cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478bea4-129e-4018-860e-bd95f98a8755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
